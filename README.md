# Efficient RAG with Learned Retrieval and Uncertainty Quantification

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/release/python-3110/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A PhD-level research implementation combining **differentiable retrieval gating** with **Bayesian uncertainty calibration** for retrieval-augmented generation (RAG).

## ğŸ¯ Research Contributions

1. **Differentiable Retrieval Gating Network**: Learned MLP router that dynamically weights BM25 vs. dense retrieval scores per passage
2. **Bayesian Confidence Calibration**: MC Dropout + Conformal Prediction hybrid for uncertainty quantification
3. **First UQ Framework for RAG**: Theoretical foundation for uncertainty in retrieval-augmented systems

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Query  â”‚â”€â”€â”€â”€â–¶â”‚           Hybrid Retrieval           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                â”‚  â”‚  BM25   â”‚â”€â”€â” â”Œâ”€â”€â”‚ ChromaDB     â”‚ â”‚
                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚  â”‚ (Dense)      â”‚ â”‚
                â”‚               â–¼ â–¼  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
                â”‚         â”‚Learned Router â”‚            â”‚
                â”‚         â”‚  (0.5M params)â”‚            â”‚
                â”‚         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   LLM (Llama 3.2 3B)   â”‚
                    â”‚   + MC Dropout UQ      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Conformal Prediction  â”‚
                    â”‚  (Coverage Guarantee)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Answer + Confidence   â”‚
                    â”‚  Interval              â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- 8GB RAM minimum (16GB recommended)
- ~10GB disk space

### Setup

```bash
# Clone repository
git clone https://github.com/yourusername/rag_uq.git
cd rag_uq

# Run setup script
chmod +x scripts/setup.sh
./scripts/setup.sh
```

### Run Full Experiment

```bash
# Full experiment (~4 hours on CPU)
docker-compose run --rm rag_uq bash scripts/run_experiment.sh

# Quick test (~30 minutes)
docker-compose run --rm rag_uq bash scripts/run_experiment.sh --quick
```

### Manual Steps

```bash
# 1. Prepare corpus
docker-compose run --rm rag_uq python data/preprocessing/prepare_corpus.py --n-articles 1000

# 2. Build index
docker-compose run --rm rag_uq python data/preprocessing/build_chroma_index.py

# 3. Train router
docker-compose run --rm rag_uq python experiments/run_router_training.py --epochs 50

# 4. Run calibration
docker-compose run --rm rag_uq python experiments/run_calibration.py --n-samples 500

# 5. Evaluate
docker-compose run --rm rag_uq python experiments/run_evaluation.py
```

## ğŸ“ Project Structure

```
rag_uq/
â”œâ”€â”€ rag_uq/                    # Core library
â”‚   â”œâ”€â”€ router.py              # Learned retrieval selector
â”‚   â”œâ”€â”€ confidence.py          # MC Dropout + Conformal Prediction
â”‚   â”œâ”€â”€ streaming_index.py     # Hybrid ChromaDB + BM25 retrieval
â”‚   â””â”€â”€ eval_protocol.py       # Calibrated evaluation metrics
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ preprocessing/         # Data pipeline scripts
â”‚   â”œâ”€â”€ preprocessed/          # Processed datasets
â”‚   â””â”€â”€ raw/                   # Downloaded data
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ run_router_training.py # Router training
â”‚   â”œâ”€â”€ run_calibration.py     # Conformal calibration
â”‚   â””â”€â”€ run_evaluation.py      # Full evaluation
â”œâ”€â”€ models/
â”‚   â””â”€â”€ router_lora/           # Trained router checkpoints
â”œâ”€â”€ results/                   # Evaluation outputs
â”œâ”€â”€ scripts/                   # Setup and run scripts
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
```

## ğŸ“Š Components

### Retrieval Router

The router is a lightweight MLP (~0.5M parameters) that learns to weight retrieval scores:

```python
from rag_uq import RetrievalRouter, RouterConfig

router = RetrievalRouter(RouterConfig(hidden_dim=64))
weights = router(bm25_scores, dense_scores)  # [0, 1] per passage
hybrid_scores = weights * dense_scores + (1 - weights) * bm25_scores
```

### Uncertainty Quantification

Two-phase uncertainty estimation:

```python
from rag_uq import MCDropoutConfidence, ConformalRAG

# Phase 1: MC Dropout
mc = MCDropoutConfidence(llm_client, n_samples=10)
result = mc.get_confidence_interval(prompt, context, question)
print(f"Uncertainty: {result.uncertainty_score}")

# Phase 2: Conformal Prediction
conformal = ConformalRAG(llm_client, alpha=0.1)
conformal.calibrate(questions, contexts, answers)
result = conformal.predict_with_coverage(question, context)
print(f"Reliable: {result.is_reliable}, Coverage: {1 - result.coverage_alpha}")
```

### Evaluation Metrics

Comprehensive evaluation across four dimensions:

| Category | Metrics |
|----------|---------|
| **Retrieval** | Recall@K, MRR, NDCG@10, Router Accuracy |
| **Generation** | Exact Match, Token F1, ROUGE-L |
| **Calibration** | ECE, MCE, Brier Score, Reliability Diagram |
| **Efficiency** | Latency (avg/p95/p99), Throughput |

## ğŸ§ª Experiments

### Router Training

Uses ApproxNDCG listwise ranking loss for end-to-end optimization:

```bash
python experiments/run_router_training.py \
    --nq-path data/preprocessed/nq_dev_3000.jsonl \
    --epochs 50 \
    --batch-size 16
```

### Conformal Calibration

Builds calibration set for coverage guarantees:

```bash
python experiments/run_calibration.py \
    --n-samples 500 \
    --model llama3.2:3b
```

## ğŸ“ˆ Results

Example results on Natural Questions (3K dev set):

| Method | EM | F1 | ECE | Avg Latency |
|--------|----|----|-----|-------------|
| BM25 only | 0.32 | 0.45 | 0.18 | 120ms |
| Dense only | 0.35 | 0.48 | 0.15 | 180ms |
| Learned Router | **0.38** | **0.52** | **0.08** | 145ms |

## ğŸ“š Theory

See [docs/uncertainty_theory.md](docs/uncertainty_theory.md) for:
- Bayesian foundations of MC Dropout
- Conformal prediction coverage guarantees
- Analysis of retrieval uncertainty propagation

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OLLAMA_BASE_URL` | Ollama API endpoint | `http://ollama:11434` |
| `CHROMA_HOST` | ChromaDB host | `chromadb` |
| `CHROMA_PORT` | ChromaDB port | `8000` |

### Router Hyperparameters

```python
RouterConfig(
    hidden_dim=64,      # MLP hidden dimension
    dropout=0.1,        # Dropout rate
    temperature=1.0,    # ApproxNDCG temperature
    num_layers=2        # MLP depth
)
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ Citation

```bibtex
@article{yourname2024raguq,
  title={Efficient RAG with Learned Retrieval and Uncertainty Quantification},
  author={Your Name},
  journal={arXiv preprint arXiv:2024.xxxxx},
  year={2024}
}
```

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Ollama](https://ollama.ai/) for local LLM inference
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [Natural Questions](https://ai.google.com/research/NaturalQuestions) dataset
